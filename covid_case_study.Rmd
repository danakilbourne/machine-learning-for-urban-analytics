---
title: "COVID-19 Case Study"
author:
- Dana Kilbourne
- Ruthie Montella
- Aziz Al Mezraani
date: 'Due 11:59PM, Feb 17'
output:
  html_document:
    code_folding: show
    highlight: haddock
    number_sections: yes
    theme: lumen
    toc: yes
    toc_depth: 4
    toc_float: yes
  pdf_document:
    extra_dependencies: ["dcolumn"]
    number_sections: yes
    toc: yes
    toc_depth: '4'
  word_document:
    toc: yes
    toc_depth: '4'
urlcolor: blue
---

---

```{r Setup, include=FALSE, results='hide', warning=FALSE}
knitr::opts_chunk$set(echo = T, fig.width=8, fig.height=4)
options(scipen = 0, digits = 3)  # controls base R output

# Package setup
if(!require("pacman")) install.packages("pacman")

pacman::p_load(tidyverse, dplyr, ggplot2, data.table, lubridate, usmap, car)
```

# Background

The outbreak of the novel Corona virus disease 2019 (COVID-19) [was declared a public health emergency of international concern by the World Health Organization (WHO) on January 30, 2020](https://www.who.int/dg/speeches/detail/who-director-general-s-statement-on-ihr-emergency-committee-on-novel-coronavirus-(2019-ncov)). Upwards of [755 million cases have been confirmed worldwide, with nearly 6.8 million associated deaths](https://covid19.who.int/) by February 2023. Within the US alone, there have been [over 1.1 million deaths and upwards of 102 million cases reported](https://covid.cdc.gov/covid-data-tracker/#trends_dailytrendscases) by February of 2023. Governments around the world have implemented and suggested a number of policies to lessen the spread of the pandemic, including mask-wearing requirements, travel restrictions, business and school closures, and even stay-at-home orders. The global pandemic has impacted the lives of individuals in countless ways, and though many countries have begun vaccinating individuals, the long-term impact of the virus remains unclear.

The impact of COVID-19 on a given segment of the population appears to vary drastically based on the socioeconomic characteristics of the segment. In particular, differing rates of infection and fatalities have been reported among different [racial groups](https://www.cdc.gov/coronavirus/2019-ncov/covid-data/investigations-discovery/hospitalization-death-by-race-ethnicity.html), [age groups](https://www.cdc.gov/coronavirus/2019-ncov/covid-data/investigations-discovery/hospitalization-death-by-age.html), and [socioeconomic groups](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7221360/). One of the most important metrics for determining the impact of the pandemic is the death rate, which is the proportion of people within the total population that die due to the the disease. 

We assembled this dataset for our research with the goal of investigating the effectiveness of lockdowns in flattening the COVID curve. We are providing a portion of the cleaned dataset for this case study.

There are two main goals for this case study:

1. We aim to show the dynamic evolution of COVID cases and COVID-related deaths at the state level.

2. We aim to identify county-level demographic and policy interventions that are associated with mortality rates in the US. We will construct models to find possible factors related to county-level COVID-19 mortality rates.

3. This is a rather complex project, but with our team's help, we have made your job easier.

4. Please hide all unnecessary lengthy R-output and keep your write-up neat and readable.
  
**Remark 1:** The data and the statistics reported here were collected before February of 2021. 

**Remark 2:** A group of RAs spent tremendous amount of time working together to assemble the data. It requires data wrangling skills. 


# Data Summary

The data comes from several different sources: 

1. [County-level infection and fatality data](https://github.com/nytimes/covid-19-data) - This dataset gives daily cumulative numbers on infection and fatality for each county. 
    * [NYC data](https://github.com/nychealth/coronavirus-data)
2. [County-level socioeconomic data](https://www.ers.usda.gov/data-products/atlas-of-rural-and-small-town-america/download-the-data/) - The following are the four relevant datasets from this site.   
    i. Income - Poverty level and household income. 
    ii. Jobs - Employment type, rate, and change.
    iii. People - Population size, density, education level, race, age, household size, and migration rates.
    iv. County Classifications - Type of county (rural or urban on a rural-urban continuum scale).
3. [Intervention Policy Data](https://github.com/JieYingWu/COVID-19_US_County-level_Summaries/blob/master/data/interventions.csv) - This dataset is a manually compiled list of the dates that interventions/lockdown policies were implemented and lifted at the county level. 

# EDA

In this case study, we use the following three nearly cleaned data:

* **covid_county.csv**: County-level socialeconomic information that combines the above-mentioned 4 datasets: Income (Poverty level and household income), Jobs (Employment type, rate, and change), People (Population size, density, education level, race, age, household size, and migration rates), County Classifications
* **covid_rates.csv**: Daily cumulative numbers on infection and fatality for each county
* **covid_intervention.csv**: County-level lockdown intervention.

**Among all data, the unique identifier of county is `FIPS`.**

The cleaning procedure is attached in `Appendix 2: Data cleaning` You may go through it if you are interested or would like to make any changes. 

First read in the data.

```{r}
# county-level socialeconomic information
county_data <- fread("data/covid_county.csv") 
# county-level COVID case and death
covid_rate <- fread("data/covid_rates.csv")
# county-level lockdown dates 
covid_intervention <- fread("data/covid_intervention.csv")
```

## Understand the data

The detailed description of variables is in `Appendix 1: Data description`. Please get familiar with the variables. Summarize the two data briefly.

county_data: This data has a tremendous number of columns, almost too many to go through. There are 8 columns dedicated to income information such as poverty rate for children, poverty rate for people of all ages, median household income, per capita income. For the poverty rates there are columns that record both the rate/percentage as well as others that document the actual number of people of all ages or under 17 years of age in poverty. The next group of columns relate to employment rates leading up to 2019 and then other columns like what percentage of the labor force is in various areas of employment. There is another large subset of the data that pertains to population size, density, education and other identifiers such as age and race. These columns include total number of households, percent of non-English speaking households, percent of population 65 years old or older, and percentage of people with different education levels. Furthermore there is a variable that records percentage of foreign born residents, an international migration rate, population of the area at various census dates, and even columns recording population change rates for each race. There are many, many more columns included in this data set, but this is an overview of the data's general themes.

covid_rate: This data set contains date, county and state columns that contain that information as well as a `fips` column which is the unique code for each county (unique identifier). Additionally it contains `cases` column which records the number of cumulative COVID-19 infections and `deaths` which is the number of COVID-19 deaths.  

## COVID case trend

It is crucial to decide the right granularity for visualization and analysis. We will compare daily vs weekly total new cases by state and we will see it is hard to interpret daily report.

i) Plot **new** COVID cases in NY, WA and FL by state and by day. Any irregular pattern? What is the biggest problem of using single day data? (Hint: The number of cases and deaths in `covid_rate` are cumulative by county. To calculate the number of **new** cases by state, first use `group_by(FIPS)`, `arrange(date)`, and `lag(cum_cases, default = 0)` function to get lagged cases, and then calculate daily new cases by subtracting `cum_cases` by the lagged `cum_cases`. Finally, aggregate the daily new cases by `State` and `date` to get the daily new cases by state.)

```{r}
# your code here
library(mapview)
## get daily new cases by county (FIP)
daily <- covid_rate %>%
  group_by(FIPS) %>%
  arrange(date) %>%
  mutate(daily_new_case = cum_cases - lag(cum_cases, default = 0))

## get daily new cases by states we 
daily_plot <- daily %>%
  group_by(State, date) %>%
  summarise(daily_new_case = sum(daily_new_case, na.rm = T)) %>%
  filter(State %in% c("New York", "Washington", "Florida"))

#plot the data 
ggplot(data = daily_plot, aes( x = date, y = daily_new_case, color = State)) + 
   geom_line() +  labs(title = "Daily New Cases in NY, WA and FL", x = "Date", y = "Daily New Cases") + 
  theme_minimal()

```

ii) Create **weekly new** cases per 100k `weekly_case_per100k` by state. Plot the spaghetti plots of `weekly_case_per100k` by state. Use `TotalPopEst2019` in `county_data` as population. (Hint: similar to i), first calculate the daily new cases by county, and then calculate the weekly new cases by aggregating the daily new cases by `State` and `week`. Then calculate the total population by `State` in `county_data`.)

```{r}
###check this one - it has to be wrong

# your code here

## get daily new cases by county

# get daily new cases by state and week
weekly <- daily %>%
  group_by(State, week) %>%
  summarize(weekly_new_case = sum(daily_new_case, na.rm = T))

# get state population
pop_state <- covid_rate %>%
  distinct(FIPS, State, TotalPopEst2019) %>%
  group_by(State) %>%
  summarize(population = sum(TotalPopEst2019, na.rm = T))

## change these up
# join weekly state case with population
joined <- inner_join(weekly, pop_state, by = "State")

# calculate weekly new cases per 100k and plot using geom_line()
joined$weekly_case_per100k <- joined$weekly_new_case / joined$population * 100000

#plot the data
ggplot(data = joined, aes(x = week, y = weekly_case_per100k, color = State)) + 
  geom_line() + labs(title = "Weekly Cases per 100k by State", x = "Week", y = "Weekly New Cases per 100k")
```

iii) Summarize the COVID case trend among states based on the plot in ii). What could be the possible reasons to explain the variability? 
In this chart, the COVID  case trend among states seems to follow the same pattern. After about week 40, each state has a huge spike in cases after two smaller peaks in earlier weeks. The possible reason for this would be that after 40 weeks of quarantining, people started entering society more and more and cases spiked. Another reason could be that around week 40 is when quarantining was ending and people were going back to work, which could have caused the spike in cases.


iv) (Optional) Use `covid_intervention` to see whether the effectiveness of lock down in flattening the curve.

```{r}
# your code here
```

## COVID death trend (Optional)

i) For each month in 2020, plot the monthly deaths per 100k heatmap by state on US map. Use the same color range across months. (Hints: Set `limits` argument in `scale_fill_gradient()` or use `facet_wrap()`; use `lubridate::month()` and `lubridate::year()` to extract month and year from date; use `tidyr::complete(state, month, fill = list(new_case_per100k = NA))` to complete the missing months with no cases.)

```{r}
# your code here
## calculate daily new death
 
## calculate monthly new death by state
 
## calculate state population
 
## join state monthly death with state population
 
## calculate monthly new_death_per100k by state
 
## `usmap()` recognizes `state` instead of `State` so need to rename(state = State)

## there is no cases for some states in the first months so there is no data
## we complete the missing months with NA
# covid_monthly %>% complete(state, month, fill = list(new_death_per100k = NA))

## Plot using plot_usmap()
# plot_usmap(
#   data = tmp, 
#   regions = "state", 
#   values = "new_death_per100k", 
#   exclude = c("Hawaii", "Alaska"), 
#   color = "black") + 
#   scale_fill_continuous(
#     low = "white", high = "blue", 
#     name = "Deaths per 100k", label = scales::comma) + 
#   labs(title = "State Fatality Rate", 
#        subtitle = "Continental US States") +
#   theme(legend.position = "right") +
#   facet_wrap(~ month)
```

ii) Use `plotly` to animate the monthly maps in i). Does it reveal any systematic way to capture the dynamic changes among states? (Hints: Follow *Appendix 3: Plotly heatmap::* in Module 6 regularization lecture to plot the heatmap using `plotly`. Use `frame` argument in `add_trace()` for animation. `plotly` only recognizes abbreviation of state names. Use `unique(us_map(regions = "states") %>% select(abbr, full))` to get the abbreviation and merge with the data to get state abbreviation.)

```{r}
# your code here
```


# COVID factors

We now try to build a good parsimonious model to find possible factors related to death rate on **county** level. Let us not take time series into account for the moment and use the total number as of *Feb 1, 2021*.

i) Create the response variable `total_death_per100k` as the total of number of COVID deaths per 100k by *Feb 1, 2021*. We suggest to take log transformation as `log_death_rate = log(total_death_per100k + 1)`. Merge `log_death_rate` to `county_data` for the following analysis. (Hints: the unique identifier for counties in `county_data` and `covid_rate` is `FIPS`.)

```{r}
# your code here
## create Feb 01, 2021 subset with log_death_rate using filter() and mutate()
covid_county_0201 <- covid_rate %>%
  filter(date == "2021-02-01") %>%
  mutate(log_death_rate = log(cum_deaths/TotalPopEst2019*1e5+1)) %>%
  select(FIPS, log_death_rate)
# 
## join with county-level demographic data
covid_county_0201 <- left_join(covid_county_0201,
                               county_data,
                               by = "FIPS")

#head(covid_county_0201)
```


## Simple regression: Effect of age on death rate

i) Start with a simple regression of `log_death_rate` vs. `Age65AndOlderPct2010` and report the `summary` output. Is `Age65AndOlderPct2010` a significant variable at the .05 level? State what effect `Age65AndOlderPct2010` has on `log_death_rate`, if any, according to this model. 

```{r}
# your code here
lm_log_death_rate <- lm(log_death_rate ~ Age65AndOlderPct2010, data = covid_county_0201)

summary(lm_log_death_rate)
```
`Age65AndOlderPct2010` is not a significant variable at the 0.05 level. The coefficient of `Age65AndOlderPct2010` is 0.0075, which means that for every 1% increase in the population over 65, the the death rate increases by 0.0075.

ii) Add `PovertyAllAgesPct` on top of the variable `Age65AndOlderPct2010` to your linear model. Is `Age65AndOlderPct2010` a significant variable at the .05 level? Give a precise interpretation of the effect of `Age65AndOlderPct2010` on `log_death_rate` in this model. 

```{r}
# your code here
lm_log_death_rate_poverty <- lm(log_death_rate ~ Age65AndOlderPct2010 + PovertyAllAgesPct, data = covid_county_0201)

summary(lm_log_death_rate_poverty)

```
`Age65AndOlderPct2010` is a significant variable at the 0.05 level. Also, the coefficient of `PovertyAllAgesPct` is significant at the 0.05 level. The coefficient of `Age65AndOlderPct2010` is 0.01027, which means that for every 1% increase in the population over 65, the the death rate increases by 0.01027, assuming the poverty percentage is held constant.

iii) The estimates and 95% CI's for the coefficient of `Age65AndOlderPct2010` are different among (i) and (ii). How would you explain the difference to a non-statistician?


In the two models, we are trying to understand why the percentage of people aged 65 and older in a county relates to the death rate. The difference is that model one only looks at the age 65 and older percentage, while model two looks at the age 65 and older percentage and the poverty percentage. The difference in estimates is because the poverty percentage is a confounding variable that affects both the age 65 and older percentage and the death rate.

The second model gives a more accurate picture because with both poverty and age, we can see how much each variable affects the death rate. Without poverty, the affect of age may look weaker than it is. 


iv) (Optional) Create a model with interaction by fitting `lm(log_death_rate ~ PovertyAllAgesPct * Age65AndOlderPct2010)`. Is the interaction effect significant at .05 level? Explain the Age65AndOlderPct2010 effect (if any). 

```{r}
# your code here
```



## Categorical variable: Effect of urban on death rate

Remember that the same variable can play different roles! Take a quick look at the variable `UrbanInfluenceCode2013`, and try to use this variable in the following analyses wisely. The Urban Influence Codes divide the 3,143 counties, county equivalents, and independent cities in the United States into 12 groups. The definition of `UrbanInfluenceCode2013` is as follows.

| Code | Description |
|-----| --------------------------------------------- |
| 1	|  In large metro area of 1+ million residents	 |
| 2	|  In small metro area of less than 1 million residents |
| 3	|  Micropolitan area adjacent to large metro area	 |
| 4	|  Noncore adjacent to large metro area	 |
| 5	|  Micropolitan area adjacent to small metro area	 |
| 6	|  Noncore adjacent to small metro area and contains a town of at least 2,500 residents	 |
| 7	|  Noncore adjacent to small metro area and does not contain a town of at least 2,500 residents	 |
| 8	|  Micropolitan area not adjacent to a metro area	 |
| 9	|  Noncore adjacent to micro area and contains a town of at least 2,500 residents	 |
| 10	|  Noncore adjacent to micro area and does not contain a town of at least 2,500 residents	 |
| 11	|  Noncore not adjacent to metro or micro area and contains a town of at least 2,500 residents	 |
| 12	|  Noncore not adjacent to metro or micro area and does not contain a town of at least 2,500 residents	 |

As you can see, the larger `UrbanInfluenceCode2013` is, the small population the county has. We can interpret `UrbanInfluenceCode2013` as either a continuous (numeric) variable or a categorical variable.

i) How many counties are there for each level of `UrbanInfluenceCode2013`? (Hint: using `table()` or `group_by() + summarise()`)

```{r}
# your code here

table(covid_county_0201$UrbanInfluenceCode2013)
```

ii) Fit a model that treats `UrbanInfluenceCode2013` as a continuous/numeric variable. Is `UrbanInfluenceCode2013` significant at the 0.05 level? What effect does `UrbanInfluenceCode2013` have on `log_death_rate` in this model?

```{r}
# your code here
lm_log_death_rate_urban <- lm(log_death_rate ~ UrbanInfluenceCode2013, data = covid_county_0201)

summary(lm_log_death_rate_urban)
```
`UrbanInfluenceCode2013` is not significant at the 0.05 level. The coefficient of `UrbanInfluenceCode2013` is -0.00706, which means that for every 1% increase in `UrbanInfluenceCode2013`, the death rate decreases by 0.00706. However, since it is not statistically significant, we cannot confidently say that there is a meaningful relationship between `UrbanInfluenceCode2013` and `log_death_rate`.


iii) Fit a model that treats `UrbanInfluenceCode2013` as a categorical/factor. Is `UrbanInfluenceCode2013` significant at the .01 level? What is the effect of `UrbanInfluenceCode2013` in this model? Describe the `UrbanInfluenceCode2013` effect over `log_death_rate`. **(REMEMBER to use Anova() from the `car` package to test a categorical variable as a whole!)**

```{r}
# your code here
lm_log_death_rate_urban_cat <- lm(log_death_rate ~ factor(UrbanInfluenceCode2013), data = covid_county_0201)

summary(lm_log_death_rate_urban_cat)

library(car)

anova_result <- anova(lm_log_death_rate_urban_cat)

print(anova_result)
```
Since the p-value is less than 0.01 in the anova test, we can reject the null hypothesis and conclude that there is a statistically significant relationship between `UrbanInfluenceCode2013` and the `log_death_rate`. This indicates that there is a very strong evidence that `UrbanInfluenceCode2013` has a significant impact on the `log_death_rate`. The intercept represents the expected value of `log_death_rate` for counties int the baseline group when `UrbanInfluenceCode2013` is equal to 1. The model's intercept is 4.6023 meaning that the counties in `UrbanInfluenceCode2013` group 1 have an expected `log_death_rate` of 4.6023. Each coefficient in each coefficient estimate for levels 2 - 12 shows the change in `log_death_rate` compared to baseline group 1. 

iv) What are the fundamental differences between treating `UrbanInfluenceCode2013` as a continuous and categorical variable in your models? 

With treating `UrbanInfluenceCode2013` as a continuous variable, it is not significant at the 0.05 level. With treating it as a categorical variable it turns into a dummy variable and some of the factors are significant at the 0.01 level, but some are not significant. With anova, there is very strong evidence that `UrbanInfluenceCode2013` has a significant impact on the `log_death_rate`, while the continuous did not have a significant impact.

v) (Optional) Can you test the null hypothesis: fit0: `log_death_rate` is linear in `UrbanInfluenceCode2013` vs. fit1: `log_death_rate` relates to `UrbanInfluenceCode2013` as a categorical variable at .01 level?  

```{r}
# your code here
```



## LASSO: Final model

i) Select possible variables in `county_data` as covariates. We provide `county_data_sub`, a subset variables from `county_data`, for you to get started. Please add any potential variables as you wish. 

    a) Report missing values in your final subset of variables. 

    b) In the following anaylsis, you may ignore the missing values.

```{r}
covid_county_0201 <- covid_county_0201 %>%
  select(log_death_rate, State, Deep_Pov_All, PovertyAllAgesPct, PerCapitaInc, UnempRate2019,
         PctEmpFIRE, PctEmpConstruction, PctEmpTrans, PctEmpMining, PctEmpTrade, PctEmpInformation,
         PctEmpAgriculture, PctEmpManufacturing, PctEmpServices, PopDensity2010, OwnHomePct,
         Age65AndOlderPct2010, TotalPop25Plus, Under18Pct2010, Ed2HSDiplomaOnlyPct,
         Ed3SomeCollegePct, Ed4AssocDegreePct, Ed5CollegePlusPct, ForeignBornPct,
         Net_International_Migration_Rate_2010_2019, NetMigrationRate1019, NaturalChangeRate1019,
         TotalPopEst2019, WhiteNonHispanicPct2010, NativeAmericanNonHispanicPct2010,
         BlackNonHispanicPct2010, AsianNonHispanicPct2010, HispanicPct2010, Type_2015_Update,
         RuralUrbanContinuumCode2013, UrbanInfluenceCode2013, Perpov_1980_0711, HiCreativeClass2000,
         HiAmenity, Retirement_Destination_2015_Update) %>%
  drop_na()

#summary(covid_county_0201)
```

There are no missing values in the final subset of variables.

ii) Use LASSO to choose a parsimonious model with all available sensible county-level information. **Force in State** in the process. Why we need to force in `State`? You may use `lambda.1se` to choose a smaller model. **(REMEMBER NEVER include unique identifiers (i.e., `FIPS`, `County`) in your model!)**

```{r}
# your code here
## get X matrix and y

## grepl(): use regular expression to locate column with some pattern
## create indicators: names include State as 0; otherwise 1
 #state_ind <- !(grepl("State", colnames(X)))
### or we can hard code

## using cv.glmnet() with penalty.factor to force in State variables

library(glmnet)
library(dplyr)
set.seed(123)
y <- covid_county_0201$log_death_rate  # what to use for dependent variable now - join?

X <- model.matrix(log_death_rate~., covid_county_0201)[,-1] # removing log_death_rate

## grepl(): use regular expression to locate column with some pattern
## create indicators: names include State as 0; otherwise 1
# state_ind <- !(grepl("State", colnames(X)))
## or we can hard code


penalty_factor <- c(rep(0,48), rep(1, ncol(X)-48))

## using cv.glmnet() with penalty.factor to force in State variables
lasso_cv <- cv.glmnet(X, y, alpha = 1, penalty.factor = penalty_factor)
#coef(lasso_cv, s = "lambda.1se")
plot(lasso_cv)
```
We need to force in the state variable since it has so many levels as a categorical variable. Forcing it in makes the model consider it only as a singular option `State` encompassing all of its components. It also means that every state will be included regardless of the value for each individual state. If we don't force it in we risk some of the states with values close to 0 being accidentally dropped or excluded. 


iii) Apply backward elimination to the LASSO model from iii) to obtain a final model with all variables significant at 0.05 level. Again **force in State** in the process. **(REMEMBER to use Anova() since we have categorical variables in our model!)**

```{r}
# your code here
coef.min <- coef(lasso_cv, s = "lambda.1se")
coef.min <- coef.min[which(coef.min != 0),][-1]
var.min <- rownames(as.matrix(coef.min))
state_ind <- grepl("State", var.min)

covid_county_subset <- covid_county_0201 %>%
  select(log_death_rate, State, var.min[!state_ind]) # ! indicating we are excluding those items

full_model <- lm(log_death_rate ~ ., data = covid_county_subset)

bwd_model <- step(full_model, direction = "backward", trace = 0)
#anova(bwd_model)
# now remove all non significant variables

bwd_model2 <- update(bwd_model, .~.-PctEmpMining)
#anova(bwd_model)


bwd_model3 <- update(bwd_model2, .~.-PopDensity2010)
anova(bwd_model3) # this is our final model because all varaibles
```

iv) Perform model diagnoses on your final model to check whether the linear model assumptions are met.

```{r}
# your code here
plot(bwd_model3$fitted.values, bwd_model3$residuals,
     xlab = "Fitted Values", ylab = "Residuals",
     main = "Residuals vs Fitted")
abline(h = 0, col = "red")

# can also do a qq plot
qqnorm(bwd_model3$residuals)
qqline(bwd_model3$residuals, lwd = 4, col = 'blue')
```
Based on the residual plot, the assumption of homoscedasticity is violated. The residuals are not randomly scattered around the horizontal line at 0. The residuals mostly lie within the residual range of -2 and 2, however there are a ton of outliers on the negative side. This suggests that the variance of the residuals is not constant across all levels of the fitted values. Additionally, there may be some outliers present in the data, as some residuals are quite far from the horizontal line.

The Q-Q plot largely demonstrates the linear assumption of normality, however outside of quantiles -2 and 2, the linear assumption of normality is violated due to deviation from the reference line. 


v) It has been shown that COVID affects elderly the most. It is also claimed that the COVID death rate among African Americans and Latinos is higher. Does your analysis support these arguments?


Also, in the bwd model above, `Age65AndOlderPct2010` is significant at the 0.05 level, with a p-value of 4.2e-10. This indicates that it is a signifant variable in predicting for covid related death rate. `HispanicPct2010` is significant at the 0.05 level, with a p-value of 0.00036. This indicates that it is a significant variable in predicting for covid related death rate. `BlackNonHispanicPct2010` was not included in our final model. This variable was removed due to it's lack of significance at the 0.05 level. This indicates that `BlackNonHispanicPct2010` does not affect the COVID related death rate. 


vi) Based on your final model, summarize your findings. In particular, summarize the state effect controlling for others. Provide intervention recommendations to policy makers to reduce COVID death rate.

State has a significant effect because Df = 48 and the p-value is 2e-16. This result suggests that state is a key factor contributing to variations in the COVID death rate across different regions, even after controlling for the other demographic and socioeconomic factors. All variables in the bwd model above are significant at the 0.05 level. In terms of the state variable controlling for other variables, we understand that the other factors are of great importance, including: public health policy, access to healthcare, and population density. We recommend implementing targeted healthcare programs, specifically for vulnerable populations, to better provide care and reduce the COVID death rate. Additionally, we recommend implementing public health policies that are tailored to the specific needs of each state, as the state variable has a significant effect on the COVID death rate. Another recommendation we have would be to include economic support for vulnerable groups like financial support and job protection, which would be crucial to mitigate the socioeconomic impacts of COVID, reducing the disparities in the death rate. 

vii) What else can we do to improve our model? What other important information we may have missed? 

In order to improve our model, we could experiment with interaction terms, especially between Age, Race, and Ethnicity. As we were saying above, further data related to healthcare infrastructure, including: number of hospitals, ICU capacity, and healthcare workers per capita could help our model predict the COVID related death rate more accurately. 

# Executive summary 

Please summarize this project as follows (this should be the structure of your final project): 

* Goal of the study

* Data
  - Source and a brief description of the data
  - How do you assemble them together (mostly done by our team but you may present them as if you have done so)
  
*	Analyses
  - Methods used
  
* Findings
* Limitations

The goal of this study is to show a dynamic evolution of COVID cases and COVID-related deaths at the state level. We would like to identify county-level demographic and policy interventions that could affect the mortality rates within the United States. We used data from the New York Times and New York City Health Data in order to find county-level fatality and infections, socioeconomic standings, and county/ state interventions. These are all things that could affect the COVID related death rate during the pandemic.  

In order to perform our analysis, we used analyzing graphs, linear regression, multiple linear regression, linear regression with interaction terms, and LASSO modeling to find the best model to predict the COVID related death rate. 

We found that the state variable was significant in our model, even after controlling for other factors. We also found that the age 65 and older percentage and Hispanic percentage were significant variables in predicting for the COVID related death rate.

Our limitations include that the data and statistics reported here were collected before February of 2021. Additionally, we believe that other important variables that could affect the COVID related death rate include healthcare infrastructure, including: number of hospitals, ICU capacity, and healthcare workers per capita, none of which are portrayed in the data we used.



# Appendix 1: Data description 


A detailed summary of the variables in each data set follows:

## Infection and fatality data

* date: Date
* county: County name
* state: State name
* fips: County code that uniquely identifies a county
* cases: Number of cumulative COVID-19 infections
* deaths: Number of cumulative COVID-19 deaths

## Socioeconomic demographics

*Income*: Poverty level and household income 

* PovertyUnder18Pct: Poverty rate for children age 0-17, 2018
* Deep_Pov_All: Deep poverty, 2014-18
* Deep_Pov_Children: Deep poverty for children, 2014-18
* PovertyAllAgesPct: Poverty rate, 2018
* MedHHInc: Median household income, 2018 (In 2018 dollars
* PerCapitaInc: Per capita income in the past 12 months (In 2018 inflation adjusted dollars), 2014-18
* PovertyAllAgesNum: Number of people of all ages in poverty, 2018
* PovertyUnder18Num: Number of people age 0-17 in poverty, 2018


*Jobs*: Employment type, rate, and change 

* UnempRate2007-2019: Unemployment rate, 2007-2019
* NumEmployed2007-2019: Employed, 2007-2019
* NumUnemployed2007-2019: Unemployed, 2007-2019

* PctEmpChange1019: Percent employment change, 2010-19
* PctEmpChange1819: Percent employment change, 2018-19
* PctEmpChange0719: Percent employment change, 2007-19
* PctEmpChange0710: Percent employment change, 2007-10

* NumCivEmployed: Civilian employed population 16 years and over, 2014-18
* NumCivLaborforce2007-2019: Civilian labor force, 2007-2019


* PctEmpFIRE: Percent of the civilian labor force 16 and over  employed in finance and insurance, and real estate and rental and leasing, 2014-18
* PctEmpConstruction: Percent of the civilian labor force 16 and over employed in construction, 2014-18
* PctEmpTrans: Percent of the civilian labor force 16 and over employed in transportation, warehousing and utilities, 2014-18
* PctEmpMining: Percent of the civilian labor force 16 and over employed in mining, quarrying, oil and gas extraction, 2014-18
* PctEmpTrade: Percent of the civilian labor force 16 and over employed in wholesale and retail trade, 2014-18
* PctEmpInformation: Percent of the civilian labor force 16 and over employed in information services, 2014-18
* PctEmpAgriculture: Percent of the civilian labor force 16 and over employed in agriculture, forestry, fishing, and hunting, 2014-18
* PctEmpManufacturing: Percent of the civilian labor force 16 and over employed in manufacturing, 2014-18
* PctEmpServices: Percent of the civilian labor force 16 and over employed in services, 2014-18
* PctEmpGovt: Percent of the civilian labor force 16 and over employed in public administration, 2014-18

*People*: Population size, density, education level, race, age, household size, and migration rates

* PopDensity2010: Population density, 2010
* LandAreaSQMiles2010: Land area in square miles, 2010

* TotalHH: Total number of households, 2014-18
* TotalOccHU: Total number of occupied housing units, 2014-18
* AvgHHSize: Average household size, 2014-18
* OwnHomeNum: Number of owner occupied housing units, 2014-18
* OwnHomePct: Percent of owner occupied housing units, 2014-18
* NonEnglishHHPct: Percent of non-English speaking households of total households, 2014-18
* HH65PlusAlonePct: Percent of persons 65 or older living alone, 2014-18
* FemaleHHPct: Percent of female headed family households of total households, 2014-18
* FemaleHHNum: Number of female headed family households, 2014-18
* NonEnglishHHNum: Number of non-English speaking households, 2014-18
* HH65PlusAloneNum: Number of persons 65 years or older living alone, 2014-18

* Age65AndOlderPct2010: Percent of population 65 or older, 2010
* Age65AndOlderNum2010: Population 65 years or older, 2010
* TotalPop25Plus: Total population 25 and older, 2014-18 - 5-year average
* Under18Pct2010: Percent of population under age 18, 2010
* Under18Num2010: Population under age 18, 2010

*  Ed1LessThanHSPct: Percent of persons with no high school diploma or GED, adults 25 and over, 2014-18
* Ed2HSDiplomaOnlyPct: Percent of persons with a high school diploma or GED only, adults 25 and over, 2014-18
* Ed3SomeCollegePct: Percent of persons with some college experience, adults 25 and over, 2014-18
* Ed4AssocDegreePct: Percent of persons with an associate's degree, adults 25 and over, 2014-18
* Ed5CollegePlusPct: Percent of persons with a 4-year college degree or more, adults 25 and over, 2014-18
* Ed1LessThanHSNum: No high school, adults 25 and over, 2014-18
* Ed2HSDiplomaOnlyNum: High school only, adults 25 and over, 2014-18
* Ed3SomeCollegeNum: Some college experience, adults 25 and over, 2014-18
* Ed4AssocDegreeNum: Number of persons with an associate's degree, adults 25 and over, 2014-18
* Ed5CollegePlusNum: College degree 4-years or more, adults 25 and over, 2014-18

* ForeignBornPct: Percent of total population foreign born, 2014-18
* ForeignBornEuropePct: Percent of persons born in Europe, 2014-18
* ForeignBornMexPct: Percent of persons born in Mexico, 2014-18
* ForeignBornCentralSouthAmPct: Percent of persons born in Central or South America, 2014-18
* ForeignBornAsiaPct: Percent of persons born in Asia, 2014-18
* ForeignBornCaribPct: Percent of persons born in the Caribbean, 2014-18
* ForeignBornAfricaPct: Percent of persons born in Africa, 2014-18
* ForeignBornNum: Number of people foreign born, 2014-18
* ForeignBornCentralSouthAmNum: Number of persons born in Central or South America, 2014-18
* ForeignBornEuropeNum: Number of persons born in Europe, 2014-18
* ForeignBornMexNum: Number of persons born in Mexico, 2014-18
* ForeignBornAfricaNum: Number of persons born in Africa, 2014-18
* ForeignBornAsiaNum: Number of persons born in Asia, 2014-18
* ForeignBornCaribNum: Number of persons born in the Caribbean, 2014-18

* Net_International_Migration_Rate_2010_2019: Net international migration rate, 2010-19
* Net_International_Migration_2010_2019: Net international migration, 2010-19
* Net_International_Migration_2000_2010: Net international migration, 2000-10
* Immigration_Rate_2000_2010: Net international migration rate, 2000-10
* NetMigrationRate0010: Net migration rate, 2000-10
* NetMigrationRate1019: Net migration rate, 2010-19
* NetMigrationNum0010: Net migration, 2000-10
* NetMigration1019: Net Migration, 2010-19

* NaturalChangeRate1019: Natural population change rate, 2010-19
* NaturalChangeRate0010: Natural population change rate, 2000-10
* NaturalChangeNum0010: Natural change, 2000-10
* NaturalChange1019: Natural population change, 2010-19

* TotalPop2010: Population size 4/1/2010 Census
* TotalPopEst2010: Population size 7/1/2010
* TotalPopEst2011: Population size 7/1/2011
* TotalPopEst2012: Population size 7/1/2012
* TotalPopEst2013: Population size 7/1/2013
* TotalPopEst2014: Population size 7/1/2014
* TotalPopEst2015: Population size 7/1/2015
* TotalPopEst2016: Population size 7/1/2016
* TotalPopEst2017: Population size 7/1/2017
* TotalPopEst2018: Population size 7/1/2018
* TotalPopEst2019: Population size 7/1/2019
* TotalPopACS: Total population, 2014-18 - 5-year average
* TotalPopEstBase2010: County Population estimate base 4/1/2010

* NonHispanicAsianPopChangeRate0010: Population change rate Non-Hispanic Asian, 2000-10
* PopChangeRate1819: Population change rate, 2018-19
* PopChangeRate1019: Population change rate, 2010-19
* PopChangeRate0010: Population change rate, 2000-10
* NonHispanicNativeAmericanPopChangeRate0010: Population change rate Non-Hispanic Native American, 2000-10
* HispanicPopChangeRate0010: Population change rate Hispanic, 2000-10
* MultipleRacePopChangeRate0010: Population change rate multiple race, 2000-10
* NonHispanicWhitePopChangeRate0010: Population change rate Non-Hispanic White, 2000-10
* NonHispanicBlackPopChangeRate0010: Population change rate Non-Hispanic African American, 2000-10

* MultipleRacePct2010: Percent multiple race, 2010
* WhiteNonHispanicPct2010: Percent Non-Hispanic White, 2010
* NativeAmericanNonHispanicPct2010: Percent Non-Hispanic Native American, 2010
* BlackNonHispanicPct2010: Percent Non-Hispanic African American, 2010
* AsianNonHispanicPct2010: Percent Non-Hispanic Asian, 2010
* HispanicPct2010: Percent Hispanic, 2010
* MultipleRaceNum2010: Population size multiple race, 2010
* WhiteNonHispanicNum2010: Population size Non-Hispanic White, 2010
* BlackNonHispanicNum2010: Population size Non-Hispanic African American, 2010
* NativeAmericanNonHispanicNum2010: Population size Non-Hispanic Native American, 2010
* AsianNonHispanicNum2010: Population size Non-Hispanic Asian, 2010
* HispanicNum2010: Population size Hispanic

## County classifications

Type of county (rural or urban on a rural-urban continuum scale)

* Type_2015_Recreation_NO: Recreation counties, 2015 edition
* Type_2015_Farming_NO: Farming-dependent counties, 2015 edition
* Type_2015_Mining_NO: Mining-dependent counties, 2015 edition
* Type_2015_Government_NO: Federal/State government-dependent counties, 2015 edition
* Type_2015_Update: County typology economic types, 2015 edition
* Type_2015_Manufacturing_NO: Manufacturing-dependent counties, 2015 edition
* Type_2015_Nonspecialized_NO: Nonspecialized counties, 2015 edition
* RecreationDependent2000: Nonmetro recreation-dependent, 1997-00
* ManufacturingDependent2000: Manufacturing-dependent, 1998-00
* FarmDependent2003: Farm-dependent, 1998-00
* EconomicDependence2000: Economic dependence, 1998-00

* RuralUrbanContinuumCode2003: Rural-urban continuum code, 2003
* UrbanInfluenceCode2003: Urban influence code, 2003
* RuralUrbanContinuumCode2013: Rural-urban continuum code, 2013
* UrbanInfluenceCode2013: Urban influence code, 2013
* Noncore2013: Nonmetro noncore, outside Micropolitan and Metropolitan, 2013
* Micropolitan2013: Micropolitan, 2013
* Nonmetro2013: Nonmetro, 2013
* Metro2013: Metro, 2013
* Metro_Adjacent2013: Nonmetro, adjacent to metro area, 2013
* Noncore2003: Nonmetro noncore, outside Micropolitan and Metropolitan, 2003
* Micropolitan2003: Micropolitan, 2003
* Metro2003: Metro, 2003
* Nonmetro2003: Nonmetro, 2003
* NonmetroNotAdj2003: Nonmetro, nonadjacent to metro area, 2003
* NonmetroAdj2003: Nonmetro, adjacent to metro area, 2003

* Oil_Gas_Change: Change in the value of onshore oil and natural gas production, 2000-11
* Gas_Change: Change in the value of onshore natural gas production, 2000-11
* Oil_Change: Change in the value of onshore oil production, 2000-11

* Hipov: High poverty counties, 2014-18
* Perpov_1980_0711: Persistent poverty counties, 2015 edition
* PersistentChildPoverty_1980_2011: Persistent child poverty counties, 2015 edition
* PersistentChildPoverty2004: Persistent child poverty counties, 2004
* PersistentPoverty2000: Persistent poverty counties, 2004

* Low_Education_2015_update: Low education counties, 2015 edition
* LowEducation2000: Low education, 2000

* HiCreativeClass2000: Creative class, 2000
* HiAmenity: High natural amenities
* RetirementDestination2000: Retirement destination, 1990-00
* Low_Employment_2015_update: Low employment counties, 2015 edition
* Population_loss_2015_update: Population loss counties, 2015 edition
* Retirement_Destination_2015_Update: Retirement destination counties, 2015 edition

# Appendix 2: Data cleaning 

The raw data sets are dirty and need transforming before we can do our EDA.
It takes time and efforts to clean and merge different data sources so we provide the final output of the cleaned and merged data. The cleaning procedure is as follows. Please read through to understand what is in the cleaned data. We set `eval = data_cleaned` in the following cleaning chunks so that these cleaning chunks will only run if any of `data/covid_county.csv`, `data/covid_rates.csv` or `data/covid_intervention.csv`  does not exist.

```{r}
# Indicator to check whether the data files exist
data_cleaned <- !(file.exists("data/covid_county.csv") & 
                    file.exists("data/covid_rates.csv") & 
                    file.exists("data/covid_intervention.csv"))
```

We first read in the table using `data.table::fread()`, as we did last time.

```{r Read in covid data, eval = data_cleaned}
# COVID case/mortality rate data
covid_rates <- fread("data/us_counties.csv", na.strings = c("NA", "", "."))
nyc <- fread("data/nycdata.csv", na.strings = c("NA", "", "."))

# Socioeconomic data
income <- fread("data/income.csv", na.strings = c("NA", "", "."))
jobs <- fread("data/jobs.csv", na.strings = c("NA", "", "."))
people <- fread("data/people.csv", na.strings = c("NA", "", "."))
county_class <- fread("data/county_classifications.csv", na.strings = c("NA", "", "."))

# Internvention policy data
int_dates <- fread("data/intervention_dates.csv", na.strings = c("NA", "", "."))
```

## Clean NYC data

The original NYC data contains more information than we need. We extract only the number of cases and deaths and format it the same as the `covid_rates` data.

```{r Clean NYC data, eval = data_cleaned}
# NYC county fips matching table 
nyc_fips <- data.table(FIPS = c('36005', '36047', '36061', '36081', '36085'),
                       County = c("BX", "BK", "MN", "QN", "SI"))

# nyc case
nyc_case <- nyc[,.(date = as.Date(date_of_interest, "%m/%d/%Y"),
                   BX = BX_CASE_COUNT, 
                   BK = BK_CASE_COUNT, 
                   MN = MN_CASE_COUNT, 
                   QN = QN_CASE_COUNT, 
                   SI = SI_CASE_COUNT)]

nyc_case %<>% 
  pivot_longer(cols = BX:SI, 
               names_to = "County",
               values_to = "cases") %>%
  arrange(date) %>%
  group_by(County) %>%
  mutate(cum_cases = cumsum(cases))

# nyc death
nyc_death <- nyc[,.(date = as.Date(date_of_interest, "%m/%d/%Y"),
                   BX = BX_DEATH_COUNT, 
                   BK = BK_DEATH_COUNT, 
                   MN = MN_DEATH_COUNT, 
                   QN = QN_DEATH_COUNT, 
                   SI = SI_DEATH_COUNT)]

nyc_death %<>% 
  pivot_longer(cols = BX:SI, 
               names_to = "County",
               values_to = "deaths") %>%
  arrange(date) %>%
  group_by(County) %>%
  mutate(cum_deaths = cumsum(deaths))

nyc_rates <- merge(nyc_case, 
                   nyc_death,
                   by = c("date", "County"),
                   all.x= T)

nyc_rates <- merge(nyc_rates,
                   nyc_fips,
                   by = "County")

nyc_rates$State <- "New York"
nyc_rates %<>% 
  select(date, FIPS, County, State, cum_cases, cum_deaths) %>%
  arrange(FIPS, date)
```
  
## Continental US cases

We only consider cases and death in continental US. Alaska, Hawaii, and Puerto Rico have 02, 15, and 72 as their respective first 2 digits of their FIPS. We use the `%/%` operator for integer division to get the first 2 digits of FIPS. We also remove Virgin Islands and Northern Mariana Islands. All data of counties in NYC are aggregated as `County == "New York City"` in `covid_rates` with no FIPS, so we combine the NYC data into `covid_rate`.

```{r, eval = data_cleaned}
covid_rates <- covid_rates %>% 
  arrange(fips, date) %>%
  filter(!(fips %/% 1000 %in% c(2, 15, 72))) %>%
  filter(county != "New York City") %>%
  filter(!(state %in% c("Virgin Islands", "Northern Mariana Islands"))) %>%
  rename(FIPS = "fips",
         County = "county",
         State = "state",
         cum_cases = "cases", 
         cum_deaths = "deaths")


covid_rates$date <- as.Date(covid_rates$date)

covid_rates <- rbind(covid_rates, 
                     nyc_rates)
```

## COVID date to week

We set the week of Jan 21, 2020 (the first case of COVID case in US) as the first week (2020-01-19 to 2020-01-25).

```{r, eval = data_cleaned}
covid_rates[, week := (interval("2020-01-19", date) %/% weeks(1)) + 1]
```

## COVID infection/mortality rates

Merge the `TotalPopEst2019` variable from the demographic data with `covid_rates` by FIPS.
  
```{r, eval = data_cleaned}
covid_rates <- merge(covid_rates[!is.na(FIPS)], 
                     people[,.(FIPS = as.character(FIPS),
                                   TotalPopEst2019)],
                     by = "FIPS",  
                     all.x = TRUE)
```


## NA in COVID data

NA values in the `covid_rates` data set correspond to a county not having confirmed cases/deaths. We replace the NA values in these columns with zeros. FIPS for Kansas city, Missouri, Rhode Island and some others are missing. We drop them for the moment and output the data up to week 57 as `covid_rates.csv`.

```{r, eval = data_cleaned}
covid_rates$cum_cases[is.na(covid_rates$cum_cases)] <- 0
covid_rates$cum_deaths[is.na(covid_rates$cum_deaths)] <- 0
```

```{r, eval = data_cleaned}
fwrite(covid_rates %>% 
         filter(week < 58) %>% 
         arrange(FIPS, date), 
       "data/covid_rates.csv")
```

## Formatting date in `int_dates`

We convert the columns representing dates in `int_dates` to R Date types using `as.Date()`. 
We will need to specify that the `origin` parameter is `"0001-01-01"`. 
We output the data as `covid_intervention.csv`.

```{r, eval = data_cleaned}
int_dates <- int_dates[-1,]
date_cols <- names(int_dates)[-(1:3)]
int_dates[, (date_cols) := lapply(.SD, as.Date, origin = "0001-01-01"), 
          .SDcols = date_cols]

fwrite(int_dates, "data/covid_intervention.csv")
```

## Merge demographic data

Merge the demographic data sets by FIPS and output as `covid_county.csv`.

```{r, eval = data_cleaned}
countydata <-
  merge(x = income,
        y = merge(
          x = people,
          y = jobs,
          by = c("FIPS", "State", "County")),
        by = c("FIPS", "State", "County"),
        all = TRUE)


countydata <- 
  merge(
    x = countydata,
    y = county_class %>% rename(FIPS = FIPStxt), 
    by = c("FIPS", "State", "County"),
    all = TRUE
  )

# Check dimensions
# They are now 3279 x 208
dim(countydata)
fwrite(countydata, "data/covid_county.csv")
```

