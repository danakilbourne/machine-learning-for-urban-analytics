---
title: "Simple Regression"
author: "Urban analytics"
output:
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: '4'
  word_document:
    toc: yes
    toc_depth: '4'
  html_document:
    code_folding: show
    highlight: haddock
    number_sections: yes
    theme: lumen
    toc: yes
    toc_depth: 4
    toc_float: yes
urlcolor: blue
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, results = "hide", fig.width=8, fig.height=4)
options(scipen = 0, digits = 3)  # controls base R output
if(!require('pacman')) {
  install.packages('pacman')
}
pacman::p_load(tidyverse, ggplot2, gridExtra, ggrepel, sf, mapview)
```



\pagebreak

# Objectives {-}

Data Science is a field of science. We try to extract useful information from data. In order to use the data efficiently and correctly we must understand the data first. According to the goal of the study, combining the domain knowledge, we then design the study. In this lecture we first go through some basic explore data analysis to understand the nature of the data, some plausible relationship among the variables. 

Data mining tools have been expanded dramatically in the past 20 years. Linear model as a building block for data science is simple and powerful. We introduce/review simple linear model. The focus is to understand what is it we are modeling; how to apply the data to get the information; to understand the intrinsic variability that statistics have. 


Contents:

 0. Suggested readings:  
    + Chapter 2 and 3
    + Statistical Sleuth, Chapter 7/8 
    + Data set: `sales_lincoln_park_2019.csv`
    
 1. Case Study
 
 2. EDA
 
 3. Simple regression (**a quick review**)
    + Model specification
    + OLS estimates and properties
    + R-squared and RSE
    + Confidence intervals for coefficients
    + Prediction intervals
    + Model diagnoses
 
 4. Appendices
    

**Technical Note:**

We hide all the outputs of R chunks by setting `results = "hide"` globally using `knitr::opts_chunk$set()` in the first chunk. You can set`results = "markup"` as default or `results = "hold"` in the global setting to show outputs of every chunk; or in the chunk headers to show outputs for that chunk. Setting `results = "hold"` holds all the output pieces and push them to the end of a chunk.




\pagebreak

# Case Study: House sales price in Lincoln park, Chicago 

Chicago, the Second city and the third-largest metropolitan area in the United States, has been one of the hottest real estate markets for years, while Chicago's housing remains pretty affordable compared to other major cities in the country. In this lecture, we focus on the house sales in the Lincoln park neighborhood of Chicago in 2019. 

**Questions of interests for us:** 

+ **Q1:** Are larger homes necessarily more expensive? How much increase in value per square feet on average?
+ **Q2:** Is 2130 Cleveland Ave worth 2 million dollars while Zillow estimates ([Zestimate](https://www.zillow.com/homedetails/2130-N-Cleveland-Ave-Chicago-IL-60614/2109619817_zpid/)) its value as around 750,000?


**Data**: `sales_lincoln_park_2019.csv`, consists of sales records of houses (excluding condos) in Lincoln park in 2019. There are 180 sales records with other variables related to the property. We will focus on the following two variables: 

* `meta_sale_price`: sales price in **million dollars** 

* `char_bldg_1ksf`: building area in **1,000 square feet**

* `meta_pin`: the unique identifier for each sales, i.e., PIN (Parcel Identification Number)


To answer the questions:

1. How does `meta_sale_price` relate to the area measured by `char_bldg_1ksf`?

2. Given `char_bldg_1ksf = 4.767` (2130 Cleveland Ave's: `char_bldg_1ksf = 4.767`, `meta_sale_price = 1.93`), 
  
    + what would be the **mean** `meta_sale_price`? 
    + what do we expect the `meta_sale_price` to be for such **a** property? 
    



\pagebreak


## Explore the relationship between `char_bldg_1ksf`, and `meta_sale_price`.

**Data preparation:**

```{r }
sales <- read.csv("data/sales_lincoln_park_2019.csv")  
names(sales)
dim(sales)
```


Let's find out the sales record of 2130 Cleveland Ave.

```{r }
sales_clev <- sales %>% filter(loc_property_address=="2130 N CLEVELAND AVE") %>% 
  select(meta_pin, loc_property_address, meta_sale_price, char_bldg_1ksf)
sales_clev
```

We would normally do a thorough EDA. We skip that portion of the data analysis and get to the regression problem directly. 

**Scatter plots** show the relationship between $x$ variable `char_bldg_1ksf` and $y$ variable `meta_sale_price`. 

```{r fig.show="hide"}
plot(x = sales$char_bldg_1ksf, 
     y = sales$meta_sale_price, 
     pch  = 16,     # "point character": shape/character of points 
     cex  = 0.8,    # size
     col  = "blue", # color 
     xlab = "Square feet (1k)",  # x-axis
     ylab = "Sales price (million)",  # y-axis 
     main = "Lincoln park's Overall Sales price vs. Square feet (1k)") # title
text(sales$char_bldg_1ksf, sales$meta_sale_price, labels=sales$loc_property_address, cex=0.7, pos=1) # label all points
```
We notice the positive association: when `char_bldg_1ksf` increases, so does `meta_sale_price`. 

`ggplot`
```{r}
ggplot(sales, aes(x = char_bldg_1ksf, y = meta_sale_price, label=loc_property_address)) + 
  geom_point(color = "blue") + 
  geom_text_repel(size=3) +
  labs(title = "Lincoln park's Overall Sales price  vs. Square feet (1k)", x = "Square feet (1k)", y = "Sales price") +
  geom_text_repel(data = sales_clev, size=3) +
  geom_point(data = sales_clev, color="red") +
  theme_bw()
```


Let's take at look at the sales price on the map.

```{r}
sales_sf <- st_as_sf(sales, coords = c("loc_longitude", "loc_latitude"), crs = 4326)
mapview(sales_sf, zcol = "meta_sale_price", layer.name = "Price (million)")
```


\pagebreak

# Simple Linear Regression

Often we would like to explore the relationship between two variables. Will a property perform better when they are paid more? The simplest model is a linear model. Let the response $y_i$ be the `meta_sale_price` and the explanatory variable $x_i$ be `char_bldg_1ksf` ($i = 1, \dots, n=180$).

Assume there is linear relationship between `meta_sale_price` and `char_bldg_1ksf`, i.e.

$$y_i = \beta_0 + \beta_1 x_i + \epsilon_i$$

**Model interpretation:**

* We assume that given `char_bldg_1ksf`, on average the `meta_sale_price` is a linear function
* For each property the `meta_sale_price` is the average plus an error term
* Parameters of interest
    + intercept: $\beta_0$
    + slope: $\beta_1$
    + both are unknown

**Estimation**

Once we have produce an estimate $(\hat{\beta}_0, \hat{\beta}_1)$, we can then 

* Interpret the slope
* Estimate the mean `meta_sale_price` and 
* Predict `meta_sale_price` for a property based on the `char_bldg_1ksf`

$$\hat y_i = \hat{\beta}_0 + \hat{\beta}_1 x_i.$$

How to estimate the parameters using the data we have?
For example, how would you decide on the following three estimates? 

```{r echo=F}
par(mfrow=c(1,3))
# Mean of char_bldg_1ksf
b0 <- mean(sales$meta_sale_price)
plot(sales$char_bldg_1ksf, sales$meta_sale_price, 
     pch  = 16, 
     xlab = "Square feet (1k)", 
     ylab = "Sales price",
     main = substitute(paste(hat(y), " = ", h), 
                       list(h=b0)))
abline(h=b0, lwd=5, col="blue")
segments(sales$char_bldg_1ksf, 
         sales$meta_sale_price, 
         sales$char_bldg_1ksf, 
         mean(sales$meta_sale_price), 
         col="blue")

# OLS
fit0 <- lm(meta_sale_price~char_bldg_1ksf, sales)
plot(sales$char_bldg_1ksf, sales$meta_sale_price, 
     pch  = 16, 
     xlab = "Square feet (1k)", 
     ylab = "Sales price",
     main = substitute(paste(hat(y), " = ", b0, "+", b1, "x"), 
                       list(b0 = fit0$coefficients[1],
                            b1 = fit0$coefficients[2])))
abline(fit0, col="red", lwd=4)
segments(sales$char_bldg_1ksf, 
         sales$meta_sale_price, 
         sales$char_bldg_1ksf, 
         fitted(fit0), 
         col="red")

# Random pick
b0 <- 0.4
b1 <- 0.08
plot(sales$char_bldg_1ksf, sales$meta_sale_price, 
     pch  = 16, 
     xlab = "Square feet (1k)", 
     ylab = "Sales price",
     main = substitute(paste(hat(y), " = ", b0, "+", b1, "x"), 
                       list(b0 = b0,
                            b1 = b1)))
abline(a = b0, b = b1, lwd=5, col="green")
segments(sales$char_bldg_1ksf, 
         sales$meta_sale_price, 
         sales$char_bldg_1ksf, 
         b0 + b1*sales$char_bldg_1ksf, 
         col="green")
```


## Ordinary least squares (OLS) estimates

Given an estimate $(b_0, b_1)$, we first define residuals as the differences between actual and predicted values of the response $y$, i.e.

$$\hat{\epsilon}_i = \hat{y}_i -b_0 - b_1 x_i.$$
In previous plots, the residuals are the vertical lines between observations and the fitted line. Now we are ready to define the OLS estimate.

The OLS estimates $\hat{\beta}_0$ and $\hat{\beta}_1$ are obtained by minimizing sum of squared errors (RSS):

$$(\hat{\beta}_0, \hat{\beta}_1) = \arg\min_{b_0,\,b_1} \sum_{i=1}^{n}\hat{\epsilon}_i^2 = \arg\min_{b_0,\,b_1} \sum_{i=1}^{n} (y_i - b_0 - b_1  x_i)^{2}.$$

We can derive the solution of $\hat{\beta}_0$ and $\hat{\beta}_1$.

$$\begin{split} 
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x}  \\
\hat{\beta}_1 &= r_{xy} \cdot \frac{s_y}{s_x}
\end{split}$$


where 

* $\bar{x}=$ sample mean of $x$'s (char_bldg_1ksf)
* $\bar{y}=$ sample mean of $y$'s (meta_sale_price)
* $s_{x}=$ sample standard deviation of $x$'s (char_bldg_1ksf)
* $s_{y}=$ sample standard deviation of $y$'s (meta_sale_price)
* $r_{xy}=$ sample correlation between $x$ and $y$.

The following shiny application shows how the fitted line and RSS change as we change $(b_0, b_1)$.
(The following chunk is not required but for the purpose of demonstration. **RUN THE CHUNK IN THE CONSOLE** to let R session to serve as a server (Copy the chunk and paste in the Console). If you are interested, [this](https://deanattali.com/blog/building-shiny-apps-tutorial/) is a step-by-step tutorial to get you started.)

```{r echo=F, eval=F}
pacman::p_load(shiny)

if(interactive()) {
  ui <- fluidPage(
    withMathJax(),
    titlePanel("Regression Model"),
    br(),
    p("This shiny application demonstrates how the regression line changes as b0 and b1 change. You can adjust b0 and b1 in the panel on the left. You can also choose two special regression line: OLS and the null model."),
    p("The red line is the plot of the OLS and the blue one is the model with the current b0 and b1. The current model and RSS is shown below the plot. You can trace the recent 10 models (b0, b1 and RSS) in the table at the bottom."),
    sidebarLayout(
      sidebarPanel(
        uiOutput("ui"),
        br(),
        p("You can change the regression line by adjusting b0 and b1"),
        br(), 
        actionButton("ols", label = helpText("\\(OLS\\)")),
        actionButton("b1zero", label = helpText("\\(b_1 = 0\\)"))
      ),
      mainPanel(
        plotOutput("lmplot"),
        br(),
        p("Current model:"),
        uiOutput("formula"),
        p("RSS:"),
        verbatimTextOutput("RSS"),
        tableOutput("history")
      )
    ))
  
  server <- function(input, output, session) {
    fit0 <- lm(meta_sale_price ~ char_bldg_1ksf, sales)
    
    # ls plot
    int_plot <- reactive({
      b0 <- input$beta0
      b1 <- input$beta1
      plot(sales$char_bldg_1ksf, sales$meta_sale_price, main="Sales price vs Square feet (1k)",
           xlab="Sales price", ylab="Square feet (1k)", pch=19)
      abline(fit0, col="red")
      abline(a = b0, b = b1, col="blue")
      if(!is.null(b0)) segments(sales$char_bldg_1ksf, 
                                sales$meta_sale_price, 
                                sales$char_bldg_1ksf, 
                                b0+b1*sales$char_bldg_1ksf, 
                                col="blue")
      legend("bottomright", legend=c(paste0(round(fit0$coefficients[1], 3), 
                                            "+", 
                                            round(fit0$coefficients[2], 3), 
                                            "x (OLS)"),
                                     paste0(b0, "+", b1, "x")),
             col=c("red", "blue"), lty=1, cex=1.5)
    })
    
    # track history
    hist_tab <- reactiveValues(hist = NULL)
    
    # outputs
    output$ui <- renderUI({
      tagList(
        sliderInput("beta0", label = h4("$$b_0$$"),
                    min = 0.3, max = 0.7, value = .7),
        sliderInput("beta1", label = h4("$$b_1$$"),
                    min = -0.2, max = 0.7, value = .2)
      )
    })
    
    output$formula <- renderUI({
      withMathJax(paste0("$$\\hat{y} = ", input$beta0, "+", input$beta1, "\\cdot x$$"))
    })
    
    output$lmplot <- renderPlot({
      int_plot()
    })
    
    output$history <- renderTable({
      if(!is.null(hist_tab$hist)) {
        ret <- data.frame(hist_tab$hist, 
                          row.names = c("b0", "b1", "RSS")) 
        colnames(ret) <- paste("M", 1:ncol(hist_tab$hist), sep = "")
        ret[,max(1, ncol(hist_tab$hist) - 9):ncol(hist_tab$hist)]
      }
    }, include.rownames=T, digits = 4)  
    
    # ols button
    observeEvent(input$ols, {
      updateSliderInput(session, "beta0", value = as.numeric(fit0$coefficients[1]))
      updateSliderInput(session, "beta1", value = as.numeric(fit0$coefficients[2]))
      int_plot()
    })
    
    # b1=0 button
    observeEvent(input$b1zero, {
      updateSliderInput(session, "beta0", value = mean(sales$meta_sale_price))
      updateSliderInput(session, "beta1", value = 0)
      int_plot
    })
    
    # trigger history
    observeEvent(c(input$beta0, input$beta1), {
      RSS <- sum((sales$meta_sale_price - (input$beta0 + input$beta1*sales$char_bldg_1ksf))^2)
      output$RSS <- renderPrint(RSS)
      hist_tab$hist <- cbind(hist_tab$hist, c(input$beta0, input$beta1, RSS))
    })
  }
  
  shinyApp(ui = ui, server = server)
}
```


### `lm()`

The function `lm()` will be used extensively. This function solves the minimization problem that we defined above. Below we use `meta_sale_price` as the dependent $y$ variable and `char_bldg_1ksf` as our $x$.

As we can see from the below output, this function outputs a list of many statistics. We will define these statistics later
```{r lm}
str(sales)
myfit0 <- lm(meta_sale_price ~ char_bldg_1ksf, data=sales)
names(myfit0)
names(summary(myfit0))
summary(myfit0)
summary(myfit0)$sigma
plot(myfit0, 1)
plot(myfit0, 2)
```  


We can also view a summary of the `lm()` output by using the `summary()` command.
```{r summary, results="hold"}
summary(myfit0)   # it is another object that is often used
results <- summary(myfit0)
names(results) 
```


Notice that the outputs of `myfit0` and `summary(myfit0)` are different
```{r results="hold"}
myfit0
b0 <- myfit0$coefficients[1]
b1 <- myfit0$coefficients[2]
```

To summarize the OLS estimate, $\hat{\beta}_0 =$ `r b0` and $\hat{\beta}_1 =$ `r b1`, we have the following estimator:

$$\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i = 0.318 + 0.416 \cdot x_i$$

Here are what we can say:

**Interpretation of the slope**:

* When `char_bldg_1ksf` increases by 1 unit (1 billion), we expect, on average the `meta_sale_price` will increase about `r  myfit0$coefficients[2]`. 

* When `char_bldg_1ksf` increases by .5 unit (500 million), we expect, on average the `meta_sale_price` will increase about `r  .5*myfit0$coefficients[2]`. 

**Prediction equation**: 

* For all the property similar to 2130 Cleveland whose `char_bldg_1ksf` is `r sales$char_bldg_1ksf[sales$loc_property_address == "2130 N CLEVELAND AVE"]`, we estimate on average the `meta_sale_price` to be

$$\hat{y}_{\text{2130 Cleveland}} = 0.318 + 0.416 \times 4.77 = 2.3$$
Or we can inline the solution from the function output:


$$\hat{y}_{\text{2130 Cleveland}} = `r myfit0$coefficients[1]` + `r myfit0$coefficients[2]` \times `r sales$char_bldg_1ksf[sales$loc_property_address == "2130 N CLEVELAND AVE"]`.$$



* **Residuals**:  For 2130 Cleveland, the real `meta_sale_price` is `r sales$char_bldg_1ksf[sales$loc_property_address == "2130 N CLEVELAND AVE"]`. So the residual for 2130 Cleveland is 


$$\hat{\epsilon}_{\text{Oakland}}= 1.93  - 2.3 = -.37$$ or 


<!-- $$\hat{\epsilon}_i = y_i - \hat{y_i} = `r sales$meta_sale_price[sales$loc_property_address == "2130 N CLEVELAND AVE"]` - `r myfit0$coefficients[1] + myfit0$coefficients[2] *  sales$char_bldg_1ksf[sales$loc_property_address == "2130 N CLEVELAND AVE"]` =  -->
<!-- `r sales$char_bldg_1ksf[sales$loc_property_address == "2130 N CLEVELAND AVE"] - myfit0$coefficients[1] +  myfit0$coefficients[2] *  sales$char_bldg_1ksf[sales$loc_property_address == "2130 N CLEVELAND AVE"]`$$ -->


Here are a few rows that show the fitted values from our model
```{r}
data.frame(sales$loc_property_address, sales$char_bldg_1ksf, sales$meta_sale_price, myfit0$fitted,
           myfit0$res)[15:25, ] # show a few rows
```

**Scatter plot with the LS line added**

Base `R`
```{r}
plot(sales$char_bldg_1ksf, sales$meta_sale_price, 
     pch  = 16, 
     xlab = "Square feet (1k)", 
     ylab = "Sales price",
     main = "Lincoln park's Overall Sales price vs. Square feet (1k)")
abline(myfit0, col="red", lwd=4)         # many other ways. 
abline(h=mean(sales$meta_sale_price), lwd=5, col="blue") # add a horizontal line, y=mean(y)
```

`ggplot`
```{r}
# find the row index of 2130 Cleveland
ggplot(sales, aes(x = char_bldg_1ksf, y = meta_sale_price) )+ 
  geom_point() + 
  geom_smooth(method="lm", se = T, color = "red") + 
  geom_hline(aes(yintercept = mean(meta_sale_price)), color = "blue") + 
  geom_text(data = sales_clev, label="2130 Cleveland", color = "red", nudge_y = -.1) +
  geom_point(data = sales_clev, color = "red") +
  labs(title = "Lincoln park's Overall Sales price  vs. Square feet (1k)", 
       x = "Square feet (1k)", y = "Sales price")
```


## Goodness of Fit: $R^2$

How well does the linear model fit the data? A common, popular notion is through $R^2$. 


**Residual Sum of Squares (RSS)**:

The least squares approach chooses $\hat{\beta}_0$ and $\hat{\beta}_1$ to minimize the RSS. RSS is defined as:

$$RSS =  \sum_{i=1}^{n} \hat{\epsilon}_i^{2} = \sum_{i=1}^{n} (y_i - \hat{\beta}_0 - \hat{\beta}_1  x_i)^{2}$$

```{r RSS, results="hold"}
myfit0 <- lm(meta_sale_price~char_bldg_1ksf, data=sales)
RSS <- sum((myfit0$res)^2) # residual sum of squares
RSS
```

**Mean Squared Error (MSE)**:

Mean Squared Error (MSE) is the average of the squares of the errors, i.e. the average squared difference between the estimated values and the actual values. For simple linear regression, MSE is defined as:

$$MSE = \frac{RSS}{n-2}.$$

**Residual Standard Error (RSE)/Root-Mean-Square-Erro(RMSE)**:

Residual Standard Error (RSE) is the square root of MSE. For simple linear regression, RSE is defined as:

$$RSE = \sqrt{MSE} = \sqrt{\frac{RSS}{n-2}}.$$

```{r RSE, results="hold"}
sqrt(RSS/myfit0$df)
summary(myfit0)$sigma
```

**Total Sum of Squares (TSS)**: 

TSS measures the total variance in the response $Y$, and can be thought of as the amount of variability inherent in the response before the regression is performed. In contrast, RSS measures the amount of variability that is left unexplained after performing the regression.

$$TSS = \sum_{i=1}^{n} (y_i - \bar{y_i})^{2}$$


```{r TSS, results="hold"}
TSS <- sum((sales$meta_sale_price-mean(sales$meta_sale_price))^2) # total sum of sqs
TSS
```

**$R^2$**: 

$R^{2}$ measures the proportion of variability in $Y$ that can be explained using $X$. An $R^{2}$ statistic that is close to 1 indicates that a large proportion of the variability in the response has been explained by the regression.

$$R^{2} =  \frac{TSS - RSS}{TSS}$$

```{r R-squre, results="hold"}
(TSS-RSS)/TSS    # Percentage reduction of the total errors
(cor(sales$meta_sale_price, myfit0$fit))^2 # Square of the cor between response and fitted values
summary(myfit0)$r.squared
```

**Remarks**:

* How large $R^2$ needs to be so that you are comfortable to use the linear model?

* Though $R^2$ is a very popular notion of goodness of fit, but it has its limitation. Mainly all the sum of squared errors defined so far are termed as `Training Errors`. It really only measures how good a model fits the data that we use to build the model. It may not generate well to unseen data. 



## Inference 

One of the most important aspect about statistics is to realize the estimators or statistics we propose such as the least squared estimators for the slope and the intercept they change as a function of data. Understanding the variability of the statistics, providing the accuracy of the estimators are one of the focus as statisticians. 

Recall that we assume a linear,
$$y_i = \beta_0 + \beta_1 x_i + \epsilon_i.$$
We did not impose assumptions on $\epsilon_i$ when using OLS. 
In order to provide some desired statistical properties and guarantees
to our OLS estimate $(\hat{\beta}_0, \hat{\beta}_1)$, we need to impose assumptions.


### Linear model assumptions

* Linearity: 

$$\textbf{E}(y_i | x_i) = \beta_0 + \beta_1 x_i$$

* homoscedasticity:

$$\textbf{Var}(y_i | x_i) = \sigma^2$$

* Normality:

$$\epsilon_i \overset{iid}{\sim} {N}(0, \sigma^2)$$
or

$$y_i \overset{iid}{\sim} {N}( \beta_0 + \beta_1 x_i, \sigma^2)$$


### Inference for the coefficients: $\beta_0$ and $\beta_1$

Under the model assumptions:

1. $y_i$ independently and identically normally distributed
2. The mean of $y$ given $x$ is linear
3. The variance of $y$ does not depend on $x$

The OLS estimates $\hat \beta = (\hat{\beta}_0, \hat{\beta}_1)$ has the following properties:

1. Unbiasedness

$$\textbf{E}(\hat{\beta}) = \beta$$

2. Normality

$$\hat{\beta}_1 \sim {N}(\beta_1, \textbf{Var}(\hat{\beta}_1))$$
where 

$$\textbf{Var}(\hat{\beta}_1) = \frac{\sigma^{2}}{x_x^2} = \frac{\sigma^{2}} {\sum_{i=1}^{n} (x_i - \bar{x})^{2}}.$$

In general,
$$\hat\beta \sim {N} (\beta,\ \sigma^2 (X^TX)^{-1})$$
Here $X$ is the design matrix where the first column is 1's and the second column is the values of x.

**Confidence intervals for the coefficients**

$t$-interval and $t$-test can be constructed using the above results. 

For example, the 95% confidence interval for $\beta$ approximately takes the form

$$\hat\beta \pm 2 \cdot SE(\hat\beta).$$

**Tests to see if the slope is 0**:

We can also perform hypothesis test on the coefficients. To be specific, we have the following test.

$$H_0: \beta_{1}=0 \mbox{ v.s. } H_1: \beta_{1} \not= 0$$

To test the null hypothesis, we need to decide whether $\hat \beta_1$ is far away from 0, 
which depends on $SE(\hat \beta_1)$. We now define the test statistics as follows.

$$t = \frac{\hat{\beta}_1 - 0}{SE(\hat \beta_1)}$$

Under the null hypothesis $\beta_{1}=0$, $t$ will have a $t$-distribution with $(n-2)$ degrees of freedom. 
Now we can compute the probability of $T\sim t_{n-2}$ equal to or larger than $|t|$, which is termed $p-value$. Roughly speaking, a small $p$-value means the odd of $\beta_{1}=0$ is small, then we can reject the null hypothesis.

$SE(\beta)$, $t$-value and $p$-value are included in the summary output.
```{r summary/t tests, results="hold"}
summary(myfit0) 
```

The `confint()` function returns the confident interval for us (95% confident interval by default).
```{r results="hold"}
confint(myfit0)
confint(myfit0, level = 0.99)
```


### Confidence for the mean response 

We use a confidence interval to quantify the uncertainty surrounding the mean of the response (`meta_sale_price`). For example, for properties like 2130 Cleveland's whose `char_bldg_1ksf=4.767`, a 95% Confidence Interval for the mean of response `meta_sale_price` is

$$\hat{y}_{|x=4.767} \pm t_{(\alpha/2, n-2)} \times \sqrt{MSE \times \left(\frac{1}{n} + \frac{(4.767-\bar{x})^2}{\sum(x_i-\bar{x})^2}\right)}.$$

The `predict()` provides prediction with confidence interval using the argument `interval="confidence"`.
```{r results="hold"}
new <- data.frame(char_bldg_1ksf=c(4.767))
CImean <- predict(myfit0, new, interval="confidence", se.fit=TRUE)  
CImean
```
Because 
$$\textbf{Var}(\hat{y}_{|x}) = \sigma^{2} \Bigg(\frac{1}{n}+\frac{(x-\bar{x})^2}{\sum_{i=1}^{n} (x_i - \bar{x})^{2}}\Bigg).$$


We can show the confidence interval for the mean response using `ggplot()` with `geom_smooth()` using the argument `se=TRUE`.
```{r results="hold"}
ggplot(sales, aes(x = char_bldg_1ksf, y = meta_sale_price)) + 
  geom_point() + 
  geom_smooth(method="lm", se = TRUE, level = 0.95, color = "red") + 
  geom_hline(aes(yintercept = mean(meta_sale_price)), color = "blue") + 
  geom_text(data = sales_clev, label="2130 Cleveland's",color="red", nudge_x = .7) +
  geom_point(data = sales_clev, color="red") +
  labs(title = "Lincoln park's Overall Sales price  vs. Square feet (1k)", 
       x = "Square feet (1k)", y = "Sales price")
```

### Prediction interval for a response

A prediction interval can be used to quantify the uncertainty surrounding `meta_sale_price` for a **particular** property.

$$\hat{y}_{|x} \pm t_{(\alpha/2, n-2)} \times \sqrt{MSE \times \left(   1+\frac{1}{n} + \frac{(x-\bar{x})^2}{\sum(x_i-\bar{x})^2}\right)}$$

We now produce 95% & 99% PI for a future $y$ given $x=4.767$ using
`predict()` again but with the argument `interval="prediction"`.

```{r results="hold"}
new <- data.frame(char_bldg_1ksf=c(4.767))
CIpred <- predict(myfit0, new, interval="prediction", se.fit=TRUE)
CIpred 
         
CIpred_99 <- predict(myfit0, new, interval="prediction", se.fit=TRUE, level=.99)
CIpred_99
```

Now we plot the confidence interval (shaded) along with the 95% prediction interval in blue and 99% prediction interval in green.
```{r}
pred_int <- predict(myfit0, interval="prediction")
colnames(pred_int) <- c("fit_95", "lwr_95", "upr_95")
pred_int_99 <- predict(myfit0, interval="prediction", level = .99)
colnames(pred_int_99) <- c("fit_99", "lwr_99", "upr_99")

cbind(sales, pred_int, pred_int_99) %>%
ggplot(aes(x = char_bldg_1ksf, y = meta_sale_price)) + 
  geom_point() + 
  geom_smooth(method="lm", se = TRUE, level = 0.95, color = "red") + 
  geom_line(aes(y=lwr_95), color = "blue", linetype = "dashed") + 
  geom_line(aes(y=upr_95), color = "blue", linetype = "dashed") + 
  geom_line(aes(y=lwr_99), color = "green", linetype = "dashed") + 
  geom_line(aes(y=upr_99), color = "green", linetype = "dashed") + 
  geom_text(data = sales_clev, label="2130 Cleveland's",color="red", nudge_x = .7) +
  geom_point(data = sales_clev, color="red") +
  labs(title = "Lincoln park's Overall Sales price  vs. Square feet (1k)", 
       x = "Square feet (1k)", y = "Sales price")
```

From our output above, the 95% prediction interval varies from `r CIpred$fit[2]` to `r CIpred$fit[3]` for a property like the 2130 Cleveland's. But its `meta_sale_price` is 1.9 million. So it is somewhat unusually low but not that unusual! 


**Read page 82 of ILSR to fully understand the difference between confidence and prediction intervals.**
 
## Model diagnoses

How reliable our confidence intervals and the tests are? We will need to check the model assumptions in the following steps:

1. Check **linearity** first; if linearity is satisfied, then

2. Check **homoscedasticity**; if homoscedasticity is satisfied, then

3. Check **normality**.


### Residual plot

We plot the residuals against the fitted values to

* check **linearity** by checking whether the residuals follow a symmetric pattern with respect to $h=0$.

* check **homoscedasticity** by checking whether the residuals are evenly distributed within a band.

```{r results="hold"}
plot(myfit0$fitted, myfit0$residuals, 
     pch  = 16,
     main = "residual plot")
abline(h=0, lwd=4, col="red")   # plot(myfit0, 1)
```


### Check normality

We look at the qqplot of residuals to check normality.

```{r results="hold"}
  qqnorm(myfit0$residuals)
  qqline(myfit0$residuals, lwd=4, col="blue") # plot(myfit0, 2)
```
  
# Summary

We introduce simple linear regression. We study the OLS estimate with its interpretation and properties. We evaluate the OLS estimate and provide inference. It is important to perform model diagnoses before coming to any conclusion. The `lm()` function is one of the most important tools for statisticians. 

We apply simple linear regression to explore the relationship between the sales vs the size of properties in Lincoln park. We found that on average, the sales price increases .4 million per 1,000 square feet increase. We further show a property like 2130 Cleveland Ave would be worth more than 2 million dollars, while the sales price of 2130 Cleveland Ave was not unusually low.



\pagebreak

# Appendices {-}

We have put a few topics here. Some of the sections might be covered in the class. 

## Appendix 1: Log-transformation

Sometimes, we transform the response and the explanatory variables/covariates before applying regression, especially for right-skewed variables such as price. There are many ways to transform the data and `log()` is one popular way because of its interpretation. It is important to know that the meaning of the coefficient changes when we apply transformation.

The following applies `log()` on `meta_sale_price`. The interpretation of $\beta$ changes as: on average 1k square feet increases in size, the sales price increases by 31\%.

```{r}
fit_log <- lm(log(meta_sale_price)~char_bldg_1ksf, sales)
summary(fit_log)
plot(fit_log)
```


## Appendix 2: Reverse Regression

Now we understand more about regression method. If one wants to predict `char_bldg_1ksf` using `meta_sale_price` as predictor can we solve for `char_bldg_1ksf` using the LS equation above to predict `char_bldg_1ksf`?

The answer is NO, and why not?

We first plot our original model:

$$y_{meta_sale_price} = 0.42260 + 0.06137 \cdot x_{char_bldg_1ksf}$$

```{r results="hold"}
par(mgp=c(1.8,.5,0), mar=c(3,3,2,1)) 
plot(sales$char_bldg_1ksf, sales$meta_sale_price, 
     pch  = 16, 
     xlab = "Square feet (1k)", 
     ylab = "Sales price",
     main = "Lincoln park's Overall Sales price vs. Square feet (1k)")
abline(lm(meta_sale_price ~ char_bldg_1ksf, data=sales), col="red", lwd=4) 
```

Now, we reverse the regression and look at the summary output
```{r results="hold"}
myfit1 <- lm(char_bldg_1ksf~meta_sale_price, data=sales)
summary(myfit1)
```

We can now overlay our two regressions:

$$y_{sf} = -1.42 + 0.95 \cdot x_{price}$$

$$y_{price} = 0.42260 + 0.06137 \cdot x_{sf}$$
Notice that this is not the same as solving $x_{sf}$ given $y_{sf}$ from the first equation which is 
$$ x_{price} = 1/0.95 y_{sf} + 1.42/0.95 =0.344873 + .124 y_{sf} $$


```{r  results="hold"}
beta0 <- myfit1$coefficients[1]
beta1 <- myfit1$coefficients[2]
win2 <- (sales$char_bldg_1ksf - beta0) / beta1   

plot(sales$char_bldg_1ksf, sales$meta_sale_price, 
     pch  = 16, 
     xlab = "Square feet (1k)", 
     ylab = "Sales price",
     main = "Lincoln park's Overall Sales price vs. Square feet (1k)")
# text(sales$char_bldg_1ksf, sales$meta_sale_price, labels=sales$loc_property_address, cex=0.7, pos=2) # label properties
abline(lm(meta_sale_price ~ char_bldg_1ksf, data=sales), col="red", lwd=4)  
lines(sales$char_bldg_1ksf, win2, col="green", lwd=4)
legend("bottomright", legend=c("y=percent", "y=char_bldg_1ksf"),
       lty=c(1,1), lwd=c(2,2), col=c("red","green"))
```
**Conclusion: Two lines are not the same!!!!!**


We may also want to get the LS equation without 2130 Cleveland first. 

```{r results="hold"}
subdata <- sales[sales$loc_property_address!="2130 N CLEVELAND AVE", ]
myfit2 <- lm(meta_sale_price ~ char_bldg_1ksf, data=subdata)
summary(myfit2)
```

The plot below shows the effect of Oakland on our linear model.
```{r results="hold"}
plot(subdata$char_bldg_1ksf, subdata$meta_sale_price, 
     pch  = 16,
     xlab = "Square feet (1k)", ylab="Sales price",
     main = "The effect of Oakland")
lines(subdata$char_bldg_1ksf, predict(myfit2), col="blue", lwd=3)
abline(myfit0, col="red", lwd=3)
legend("bottomright", legend=c("Reg. with Oakland", "Reg. w/o Oakland"),
       lty=c(1,1), lwd=c(2,2), col=c("red","blue"))
```



## Appendix 3:  t-distribution vs z-distribution

Difference between $z$ and $t$ with $df=n$. The distribution of $z$ is similar to that $t$ when $df$ is large, say 30.
```{r results="hold"}
z <- rnorm(1000)   
hist(z, freq=FALSE, col="red", breaks=30, xlim=c(-5,5))
```

Check Normality
```{r results="hold"}
qqnorm(z, pch=16, col="blue", cex=0.7)  
qqline(z)   #shift-command-c  to add or to suppress comment
```

See what a $t$ variable looks like with a degrees of freedom at 5
```{r results="hold"}
df <-5 # you may change df: df is large then t is approx same as z
t <- rt(1000, df)   # 
hist(t, freq=FALSE, col="blue", breaks=50, xlim=c(-5,5),
     main=paste("Hist of t with df=",df))
qqnorm(t)
qqline(t)
```


Make a small set of graphs to see the variability of sample from sample
```{r }
# Put graphs into 2 rows and 2 cols
par(mfrow=c(2,2))

for (i in 1:4){
  z <- rnorm(1000)   
  par(mgp=c(1.8,.5,0), mar=c(3,3,2,1)) 
  hist(z, freq=FALSE, col="red", breaks=30, xlim=c(-5,5))

  qqnorm(z, pch=16, col="blue", cex=0.7)    # check normality
  qqline(z)
}
```




  

## Appendix 4: Investigate R-Squared

### Case I: Non-linear response

A perfect model between X and Y but it is not linear. $R^{2}=.837$  here $y=x^{3}$ with no noise!  

```{r results="hold"}
x <- seq(0, 3, by=.05) # or x=seq(0, 3, length.out=61)
y <- x^3 # no variability

myfit <- lm(y ~ x)
myfit.out <- summary(myfit)
rsquared <- myfit.out$r.squared

plot(x, y, pch=16, ylab="",
   xlab = "No noise in the data",
   main = paste("R squared = ", round(rsquared, 3), sep=""))
abline(myfit, col="red", lwd=4)
```


### Case II: Large variance vs small variance
A perfect linear model between X and Y but with noise here $y= 2+3x + \epsilon$, $\epsilon \overset{iid}{\sim} N(0,9)$. Run this repeatedly

```{r results="hold"}
par(mfrow = c(2,2))

for(i in 1:4){
  x <- seq(0, 3, by=.02)
  e <- 3*rnorm(length(x))   # Normal random errors with mean 0 and sigma=3
  y <- 2 + 3*x + 3*rnorm(length(x)) 
  
  myfit <- lm(y ~ x)
  myfit.out <- summary(myfit)
  rsquared <- round(myfit.out$r.squared,3)
  hat_beta_0 <- round(myfit$coe[1], 2)
  hat_beta_1 <- round(myfit$coe[2], 2)
  par(mgp=c(1.8,.5,0), mar=c(3,3,2,1)) 
  plot(x, y, pch=16, ylab="",
       xlab = "True lindear model with errors", 
       main = paste("R squared= ", rsquared, 
                   "LS est's=", hat_beta_0, "and", hat_beta_1))
  
  abline(myfit, col="red", lwd=4)
}
```


Similar setup but with smaller variance $\epsilon \overset{iid}{\sim} N(0,1)$.

```{r results="hold"}
par(mfrow = c(2,2))

for(i in 1:4){
  x <- seq(0, 3, by=.02)
  e <- 3*rnorm(length(x))   # Normal random errors with mean 0 and sigma=3
  y <- 2 + 3*x + 1*rnorm(length(x)) 
  
  myfit <- lm(y ~ x)
  myfit.out <- summary(myfit)
  rsquared <- round(myfit.out$r.squared, 3)
  b1 <- round(myfit.out$coe[2], 3)
  par(mgp=c(1.8,.5,0), mar=c(3,3,2,1)) 
  plot(x, y, pch=16, 
       ylab = "",
       xlab = paste("LS estimates, b1=", b1, ",  R^2=", rsquared),
       main = "The true model is y=2+3x+n(0,1)")
  abline(myfit, col="red", lwd=4)
}
```



##  Appendix 5: More on Model Diagnoses
What do we expect to see even all the model assumptions are met?

  * a) Variability of the ls estimates $\beta$'s
  * b) Variability of the $R^{2}$'s
  * c) Variability of the $\hat \sigma^{2}$'s
  * d) Model diagnoses: through residuals 

We demonstrate this through a simulation.

Here is a case that all the linear model assumptions are met. Once again everything can be checked by examining the residual plots

1. Randomize X
```{r }
## Set up the simulations
set.seed(1) # set seed so that x will be the same each time we run it
x <- runif(100) # generate 100 random numbers from [0, 1], hist(x)
hist(x)
```

2. Generate y only each time to see the variability of ls estimates. The true $y$ is

$$y = 1 +2x + {N}(0,2^2) \quad i.e. \quad \beta_0 = 1 ,\beta_1 = 2, \sigma^2 = 4.$$
```{r }
# mar: c(bottom, left, top, right) to specify the margin on four sides
# mgp: specify the margin for axis title, axis labels and axis line

# par(mfrow=c(2,2), mar=c(2,2,2,2), mgp=c(2,0.5,0))

# for (i in 1:4) {
  y <- 1 + 2*x + rnorm(100, 0, 2) # generate response y's. may change n
  
  fit <- lm(y ~ x)
  fit.perfect <- summary(lm(y ~ x))
  rsquared <- round(fit.perfect$r.squared, 2)
  hat_beta_0 <- round(fit.perfect$coefficients[1], 2)
  hat_beta_1 <- round(fit.perfect$coefficients[2], 2)
hat_sigma  <- round(fit.perfect$sigma, 2)
plot(x, y, pch=16, 
     ylim=c(-8,8),
     xlab="a perfect linear model: true mean: y=1+2x in blue, LS in red",
     main=paste("R squared= ",rsquared, ",",
                 "hat sig=", hat_sigma, ",",  "LS  b1=",hat_beta_1, ",",  "and b0=", hat_beta_0))
abline(fit, lwd=4, col="red")
lines(x, 1+2*x, lwd=4, col="blue")
  # 
  # 
  # plot(x, y, pch=i, 
  #      ylim = c(-8,8),
  #      xlab = "true mean: y=1+2x in blue, LS in red",
  #      main = paste("R^2=", rsquared, 
  #                   ", b1_LSE=", hat_beta_1, " and b0=", hat_beta_0))
  # abline(fit, lwd=4, col="red")
  # lines(x, 1 + 2*x, lwd=4, col="blue")
  # abline(h= mean(y), lwd=4, col="green")
# }
```
  
The theory says that 

$$\hat{\beta}_1 \sim {N} (\beta_1 = 2,\, \textbf{Var}(\hat{\beta}_1)).$$
 
```{r }
  sigma <- 2
  n <- length(y)
  sd_b1 <- sqrt(sigma^2 /((n-1)* (sd(x))^2))  # we will estimate sigma by rse in real life.
  sd_b1
  summary(fit)
```

 
Plots
```{r }
  plot(x, y, pch=i, 
       ylim = c(-8,8),
       xlab = "a perfect linear model: true mean: y=1+2x in blue, LS in red",
       main = paste("R squared=", rsquared, 
                    ", LS estimates b1=", hat_beta_1, " and b0=", hat_beta_0))
  abline(fit, lwd=4, col="red")
  lines(x, 1 + 2*x, lwd=4, col="blue")
  abline(h= mean(y), lwd=4, col="green")

  # Residuals
  plot(fit$fitted, fit$residuals, 
       pch  = 16,
       ylim = c(-8, 8),
       main = "residual plot")
  abline(h=0, lwd=4, col="red")
  
  # Normality
  qqnorm(fit$residuals, ylim=c(-8, 8))
  qqline(fit$residuals, lwd=4, col="blue")
```


##  Appendix 6: Sample Statistics

We remind the readers definition of sample statistics here. 


* Sample mean:

$$\bar{y} = \frac{1}{n}\sum_{i=1}^n y_i$$

* Sample variance:

$$s^2 = \frac{\sum_{i=1}^{n}(y_i - \bar{y})^2}{n-1}$$

* Sample Standard Deviation:

$$s = \sqrt\frac{\sum_{i=1}^{n}(y_i - \bar{y})^2} {n - 1}$$

* Sample correlation

$$r = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^n (x_i - \bar{x})^2 \sum_{i=1}^n (y_i - \bar{y})^2}} = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{s_x s_y} $$
